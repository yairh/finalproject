{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T13:24:36.158898Z",
     "start_time": "2019-02-05T13:24:36.146811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ds3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from class_dataset import ChestDataset\n",
    "import pandas as pd\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.applications import DenseNet121\n",
    "from keras import models\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer,OneHotEncoder,MultiLabelBinarizer\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import keras\n",
    "import cv2\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T13:03:07.191663Z",
     "start_time": "2019-02-05T13:03:07.185233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17107079016664964972\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11287530701\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3264332340083326695\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "os.system('sudo chown -R ds:ds /data')\n",
    "if not os.path.exists('./output/'):\n",
    "    os.mkdir('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T13:03:44.755675Z",
     "start_time": "2019-02-05T13:03:42.938611Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHOOSE now your model name \n",
    "model_name = 'densechest_multiclass_aug'\n",
    "\n",
    "csvfile = 'data_kaggle/Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csvfile)\n",
    "\n",
    "data_dir = '/data/xray_chest_final/'\n",
    "\n",
    "ChestDataset(data_dir,df).reset_folder()\n",
    "\n",
    "df = ChestDataset(data_dir,df).reader\n",
    "df = df[df.exists == True]\n",
    "df = df[~df['Finding Labels'].str.contains('\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['Finding Labels'].value_counts() \n",
    "labels_to_keep = count[count > 500].index.values\n",
    "df = df[df['Finding Labels'].isin(labels_to_keep)]\n",
    "df['labels'] = df['Finding Labels'].apply(lambda x: [x])\n",
    "binarizer = MultiLabelBinarizer()\n",
    "binarizer.fit(df.labels)\n",
    "df['target'] = list(binarizer.transform(df.labels))\n",
    "row_to_drop = [idx for i,idx in enumerate(df[df['Finding Labels'] == 'No Finding'].index.values) if i%3==0]\n",
    "df = df.drop(row_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T13:03:45.794230Z",
     "start_time": "2019-02-05T13:03:44.950258Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ChestDataset(data_dir,df)\n",
    "\n",
    "train_list = [el[len(data_dir):] for i,el in enumerate(dataset.image_path) if not i%5 == 0]\n",
    "test_list = [el[len(data_dir):] for i,el in enumerate(dataset.image_path) if i%5 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T13:03:52.190627Z",
     "start_time": "2019-02-05T13:03:46.704289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/notebooks/class_dataset.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.reader['exists'] = self.exists\n"
     ]
    }
   ],
   "source": [
    "with open('output/{}_train_list.txt'.format(model_name), 'w') as f:\n",
    "    for item in train_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('output/{}_test_list.txt'.format(model_name), 'w') as f:\n",
    "    for item in test_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "train_dt,test_dt = dataset.train_test(train_list,test_list)\n",
    "train_dt.create_tree()\n",
    "test_dt.create_tree()\n",
    "\n",
    "train_files = train_dt.image_path\n",
    "test_files = test_dt.image_path\n",
    "train_folder = train_dt.dir\n",
    "test_folder = test_dt.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Finding in Train:  0.6742581090407177\n",
      "No Finding in Test:  0.6859858938975775\n",
      "Infiltration in Train:  0.12253661529023847\n",
      "Infiltration in Test:  0.12634161300214658\n",
      "Atelectasis in Train:  0.07000996856069319\n",
      "Atelectasis in Test:  0.05519779208831647\n",
      "Effusion in Train:  0.05666743347902768\n",
      "Effusion in Test:  0.060410916896657466\n",
      "Nodule in Train:  0.04455179817498658\n",
      "Nodule in Test:  0.03925176326280282\n",
      "Pneumothorax in Train:  0.031976075454336325\n",
      "Pneumothorax in Test:  0.032812020852499235\n"
     ]
    }
   ],
   "source": [
    "for l in labels_to_keep:\n",
    "    print(l + ' in Train: ', train_dt.labels.count(l)/len(train_dt))\n",
    "    print(l + ' in Test: ', test_dt.labels.count(l)/len(test_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T11:32:29.102093Z",
     "start_time": "2019-02-05T11:31:49.980953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 8, 8, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 393222    \n",
      "=================================================================\n",
      "Total params: 7,430,726\n",
      "Trainable params: 7,347,078\n",
      "Non-trainable params: 83,648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ADD YOUR MODEL\n",
    "img_width,img_height = 256,256\n",
    "densenet = DenseNet121(weights='imagenet', include_top=False,input_shape = (img_width, img_height, 3))\n",
    "\n",
    "# # Freeze some layers\n",
    "# for layer in densenet.layers[:100]:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(densenet)\n",
    "\n",
    "# Add new layers\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(72))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:12:03.451647Z",
     "start_time": "2019-02-05T14:12:03.426138Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate(img,deg):\n",
    "    return ndimage.rotate(img,deg)\n",
    "\n",
    "def stdize(img):\n",
    "    means = np.array([0.485, 0.456, 0.406])\n",
    "    stds = np.array([0.229, 0.224, 0.225])\n",
    "    for i in range(img.shape[2]):\n",
    "        img[:,:,i] = (img[:,:,i] - means[i])/stds[i]\n",
    "    return img\n",
    "\n",
    "def get_input(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img,(256,256))\n",
    "    img = img/255.0\n",
    "#     img = stdize(img)\n",
    "    return img\n",
    "\n",
    "def get_label(path):\n",
    "    label = path.split('/')[-2]\n",
    "    return label\n",
    "\n",
    "def preprocess(img):\n",
    "    batch = [img]\n",
    "    \n",
    "    if np.random.normal() > 0.5:\n",
    "        img_aug = rotate(img,90)\n",
    "        batch += [img_aug]\n",
    "    if np.random.normal() > 0.5:\n",
    "        img_aug = rotate(img,180)\n",
    "        batch += [img_aug]\n",
    "    if np.random.normal() > 0.5:\n",
    "        img_aug = rotate(img,-90)    \n",
    "        batch += [img_aug]\n",
    "    if np.random.normal() > 0.5:\n",
    "        img_aug = np.flipud(img)    \n",
    "        batch += [img_aug]\n",
    "    if np.random.normal() > 0.5:\n",
    "        img_aug = np.fliplr(img)    \n",
    "        batch += [img_aug]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def ChestGen(files, batch_size = 16,augment = False):\n",
    "    shuffle(files)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        if idx + batch_size > len(files):\n",
    "            batch_paths = files[idx:]\n",
    "        else:\n",
    "            batch_paths = files[idx:idx + batch_size]\n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for input_path in batch_paths:\n",
    "            x = get_input(input_path)\n",
    "            y = get_label(input_path)\n",
    "            vec = binarizer.transform([(y,)])\n",
    "            batch_input += [x]\n",
    "            batch_output += list(vec)\n",
    "\n",
    "            if y != 'No Finding' and augment:\n",
    "                batch_prep = preprocess(x)\n",
    "                batch_input += batch_prep\n",
    "                batch_output += list(vec)*len(batch_prep)\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "        \n",
    "        idx += batch_size\n",
    "\n",
    "        yield(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:07:31.447807Z",
     "start_time": "2019-02-05T14:07:31.217131Z"
    }
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"output/{}.json\".format(model_name), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "\n",
    "train_batchsize = 13\n",
    "val_batchsize = 13\n",
    "\n",
    "train_path = glob(data_dir +'train/**/*')\n",
    "test_path = glob(data_dir + 'test/**/*')\n",
    "\n",
    "train_generator = ChestGen(train_path,train_batchsize,augment=True)\n",
    "test_generator = ChestGen(test_path,val_batchsize,augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8c8dc2998152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mval_batchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     callbacks=[tensorboard,checkpoint,lr_sc])\n\u001b[0m",
      "\u001b[0;32m/opt/ds3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2192\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2194\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds3/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 \u001b[0mall_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall_finished\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='output/logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "filepath = \"output/checkpoint_{}.hdf5\".format(model_name)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "lr_sc = ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=3,verbose=1)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_path) // train_batchsize,\n",
    "    epochs=50,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_path) // val_batchsize,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard,checkpoint,lr_sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "fig.savefig('output/history_{}.png'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"output/{}.h5\".format(model_name))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_generator(validation_generator,\n",
    "                                     steps=len(validation_generator),\n",
    "                                     pickle_safe=True,\n",
    "                                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(prediction,axis=1)\n",
    "\n",
    "y_true = np.zeros((preds.shape[0],validation_generator.num_classes))\n",
    "y_true[np.arange(preds.shape[0]), validation_generator.classes] = 1\n",
    "inv_map = {v:k for k,v in validation_generator.class_indices.items()}\n",
    "pred_cat = [inv_map[i] for i in preds]\n",
    "\n",
    "report = classification_report(validation_generator.classes,preds)\n",
    "np.save('output/report_{}.npy'.format(model_name),report)\n",
    "print(report)\n",
    "print('Accuracy score: ',accuracy_score(validation_generator.classes,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator,\n",
    "                                 steps=len(validation_generator),\n",
    "                                 pickle_safe=True)\n",
    "print('Accuracy Keras: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auc scores\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(validation_generator.num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], prediction[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for i in range(validation_generator.num_classes):\n",
    "    plt.plot(fpr[i], tpr[i],\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(inv_map[i], roc_auc[i]))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "fig.savefig('output/roc_curve_{}.png'.format(model_name))\n",
    "\n",
    "print('End Of Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
