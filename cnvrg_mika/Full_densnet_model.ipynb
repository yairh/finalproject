{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from IPython.core.display import Image, display\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16, DenseNet121\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "import time\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "# from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about the Dataset:\n",
      "\n",
      "There are 2 total chest deseases.\n",
      "There are 4999 total chest images.\n",
      "\n",
      "There are 4032 training chest images.\n",
      "There are 967 test chest images.\n"
     ]
    }
   ],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    \"\"\"Returns the path and the Label from the folder\"\"\"\n",
    "    data = load_files(path)\n",
    "    chest_files = np.array(data['filenames'])\n",
    "    chest_targets = np_utils.to_categorical(np.array(data['target']), 2)\n",
    "    return chest_files, chest_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../imgs/images/train')\n",
    "test_files, test_targets = load_dataset('../imgs/images/test')\n",
    "\n",
    "# load list of dog names\n",
    "labels = [item[21:-1] for item in sorted(glob(\"../imgs/images/train/*/\"))]\n",
    "CLASSES = len(labels)\n",
    "\n",
    "print('Statistics about the Dataset:\\n')\n",
    "print('There are %d total chest deseases.' % len(labels))\n",
    "print('There are %s total chest images.\\n' % len(np.hstack([train_files, test_files])))\n",
    "print('There are %d training chest images.' % len(train_files))\n",
    "print('There are %d test chest images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************TRAIN GENERATOR**********************\n",
      "Found 4032 images belonging to 2 classes.\n",
      "**********************TEST GENERATOR**********************\n",
      "Found 967 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height, channels = 365, 365, 3\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '../imgs/images/train'\n",
    "test_data_dir = '../imgs/images/test'\n",
    "\n",
    "\n",
    "train_batch_size = 20\n",
    "test_batch_size = 10\n",
    "\n",
    "\n",
    "print('**********************TRAIN GENERATOR**********************')\n",
    "### Train Generator\n",
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size = train_batch_size,\n",
    "                                                    class_mode = 'categorical',\n",
    "                                                    shuffle=False)\n",
    "                                                    #color_mode = 'grayscale'\n",
    "    \n",
    "\n",
    "print('**********************TEST GENERATOR**********************')\n",
    "### Test Generator\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=test_batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 11, 11, 1024)      7037504   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 123904)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               31719680  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 38,758,730\n",
      "Trainable params: 31,720,710\n",
      "Non-trainable params: 7,038,020\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = 4032\n",
    "nb_test_samples = 967\n",
    "\n",
    "# Define the architecture\n",
    "model = DenseNet121(weights= 'imagenet', include_top=False, input_shape=(img_height, img_width, channels)) # or weights=None\n",
    "\n",
    "# Freeze some layers\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "Chest_Dens_121_model = Sequential()\n",
    "Chest_Dens_121_model.add(model)\n",
    "\n",
    "Chest_Dens_121_model.add(layers.Flatten())\n",
    "\n",
    "Chest_Dens_121_model.add(layers.Dense(256))\n",
    "Chest_Dens_121_model.add(layers.BatchNormalization())\n",
    "Chest_Dens_121_model.add(layers.Activation('relu'))\n",
    "Chest_Dens_121_model.add(layers.Dropout(0.5))\n",
    "\n",
    "Chest_Dens_121_model.add(layers.Dense(2))\n",
    "Chest_Dens_121_model.add(layers.BatchNormalization())\n",
    "Chest_Dens_121_model.add(layers.Activation('softmax'))\n",
    "Chest_Dens_121_model.add(layers.Dropout(0.5))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "Chest_Dens_121_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "Chest_Dens_121_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/201 [============================>.] - ETA: 2:50:57 - loss: 6.1991 - acc: 0.45 - ETA: 2:50:52 - loss: 5.6156 - acc: 0.51 - ETA: 3:05:21 - loss: 5.6941 - acc: 0.49 - ETA: 3:10:51 - loss: 6.2298 - acc: 0.45 - ETA: 3:13:46 - loss: 5.9918 - acc: 0.46 - ETA: 3:16:01 - loss: 6.0490 - acc: 0.46 - ETA: 3:17:26 - loss: 5.9537 - acc: 0.46 - ETA: 3:18:36 - loss: 5.7955 - acc: 0.47 - ETA: 3:19:13 - loss: 5.9048 - acc: 0.47 - ETA: 3:19:33 - loss: 6.0905 - acc: 0.47 - ETA: 3:19:40 - loss: 6.0628 - acc: 0.47 - ETA: 3:19:04 - loss: 6.0776 - acc: 0.47 - ETA: 3:18:07 - loss: 6.0864 - acc: 0.47 - ETA: 3:17:10 - loss: 6.1908 - acc: 0.46 - ETA: 3:16:17 - loss: 6.2453 - acc: 0.46 - ETA: 3:15:24 - loss: 6.2931 - acc: 0.45 - ETA: 3:15:10 - loss: 6.3367 - acc: 0.45 - ETA: 3:14:35 - loss: 6.3558 - acc: 0.45 - ETA: 3:14:00 - loss: 6.3710 - acc: 0.45 - ETA: 3:13:16 - loss: 6.3217 - acc: 0.45 - ETA: 3:12:28 - loss: 6.3039 - acc: 0.45 - ETA: 3:11:21 - loss: 6.2156 - acc: 0.45 - ETA: 3:10:14 - loss: 6.1455 - acc: 0.45 - ETA: 3:09:08 - loss: 6.1984 - acc: 0.45 - ETA: 3:08:03 - loss: 6.2325 - acc: 0.44 - ETA: 3:07:07 - loss: 6.2010 - acc: 0.45 - ETA: 3:06:17 - loss: 6.2298 - acc: 0.44 - ETA: 3:05:29 - loss: 6.1164 - acc: 0.45 - ETA: 3:04:39 - loss: 6.1034 - acc: 0.45 - ETA: 3:03:40 - loss: 6.1223 - acc: 0.45 - ETA: 3:02:41 - loss: 6.0905 - acc: 0.45 - ETA: 3:01:39 - loss: 6.0480 - acc: 0.46 - ETA: 3:00:24 - loss: 6.0521 - acc: 0.46 - ETA: 2:59:26 - loss: 6.0127 - acc: 0.46 - ETA: 2:58:30 - loss: 6.0105 - acc: 0.46 - ETA: 2:57:34 - loss: 6.0383 - acc: 0.45 - ETA: 2:56:39 - loss: 6.0333 - acc: 0.45 - ETA: 2:55:44 - loss: 5.9993 - acc: 0.45 - ETA: 2:54:48 - loss: 5.9756 - acc: 0.45 - ETA: 2:53:58 - loss: 5.9747 - acc: 0.45 - ETA: 2:52:59 - loss: 6.0024 - acc: 0.45 - ETA: 2:52:00 - loss: 6.0085 - acc: 0.45 - ETA: 2:50:59 - loss: 5.9931 - acc: 0.45 - ETA: 2:50:01 - loss: 6.0354 - acc: 0.45 - ETA: 2:49:03 - loss: 6.0477 - acc: 0.45 - ETA: 2:48:07 - loss: 6.0578 - acc: 0.45 - ETA: 2:47:11 - loss: 6.0777 - acc: 0.45 - ETA: 2:46:12 - loss: 6.0956 - acc: 0.45 - ETA: 2:45:14 - loss: 6.0403 - acc: 0.45 - ETA: 2:44:17 - loss: 6.0458 - acc: 0.45 - ETA: 2:43:19 - loss: 6.0407 - acc: 0.45 - ETA: 2:42:20 - loss: 6.0308 - acc: 0.45 - ETA: 2:41:20 - loss: 6.0407 - acc: 0.45 - ETA: 2:40:20 - loss: 6.0133 - acc: 0.46 - ETA: 2:39:14 - loss: 6.0032 - acc: 0.45 - ETA: 2:38:10 - loss: 6.0204 - acc: 0.45 - ETA: 2:36:57 - loss: 5.9892 - acc: 0.46 - ETA: 2:35:50 - loss: 5.9993 - acc: 0.46 - ETA: 2:34:45 - loss: 5.9883 - acc: 0.46 - ETA: 2:33:39 - loss: 6.0122 - acc: 0.46 - ETA: 2:32:40 - loss: 6.0093 - acc: 0.46 - ETA: 2:31:38 - loss: 6.0131 - acc: 0.46 - ETA: 2:30:31 - loss: 6.0306 - acc: 0.46 - ETA: 2:29:25 - loss: 6.0089 - acc: 0.46 - ETA: 2:28:24 - loss: 6.0066 - acc: 0.46 - ETA: 2:27:37 - loss: 6.0226 - acc: 0.46 - ETA: 2:26:32 - loss: 6.0145 - acc: 0.46 - ETA: 2:25:30 - loss: 6.0304 - acc: 0.46 - ETA: 2:24:24 - loss: 6.0156 - acc: 0.46 - ETA: 2:23:17 - loss: 6.0315 - acc: 0.46 - ETA: 2:22:17 - loss: 6.0732 - acc: 0.45 - ETA: 2:21:17 - loss: 6.1012 - acc: 0.45 - ETA: 2:20:15 - loss: 6.1134 - acc: 0.45 - ETA: 2:19:15 - loss: 6.1306 - acc: 0.45 - ETA: 2:18:14 - loss: 6.1359 - acc: 0.45 - ETA: 2:17:06 - loss: 6.1335 - acc: 0.45 - ETA: 2:15:58 - loss: 6.1190 - acc: 0.45 - ETA: 2:14:53 - loss: 6.1037 - acc: 0.45 - ETA: 2:13:53 - loss: 6.1015 - acc: 0.45 - ETA: 2:12:49 - loss: 6.1047 - acc: 0.45 - ETA: 2:11:43 - loss: 6.1009 - acc: 0.45 - ETA: 2:10:39 - loss: 6.0879 - acc: 0.46 - ETA: 2:09:16 - loss: 6.0904 - acc: 0.45 - ETA: 2:07:36 - loss: 6.0921 - acc: 0.45 - ETA: 2:06:01 - loss: 6.0839 - acc: 0.46 - ETA: 2:04:27 - loss: 6.0923 - acc: 0.45 - ETA: 2:02:56 - loss: 6.0857 - acc: 0.45 - ETA: 2:01:27 - loss: 6.0782 - acc: 0.45 - ETA: 2:00:00 - loss: 6.0858 - acc: 0.45 - ETA: 1:58:33 - loss: 6.0963 - acc: 0.45 - ETA: 1:57:08 - loss: 6.0901 - acc: 0.45 - ETA: 1:55:45 - loss: 6.1096 - acc: 0.45 - ETA: 1:54:19 - loss: 6.1237 - acc: 0.45 - ETA: 1:52:55 - loss: 6.1416 - acc: 0.45 - ETA: 1:51:32 - loss: 6.1343 - acc: 0.45 - ETA: 1:50:10 - loss: 6.1508 - acc: 0.45 - ETA: 1:48:50 - loss: 6.1474 - acc: 0.45 - ETA: 1:47:30 - loss: 6.1494 - acc: 0.45 - ETA: 1:46:10 - loss: 6.1459 - acc: 0.45 - ETA: 1:44:51 - loss: 6.1664 - acc: 0.45 - ETA: 1:43:32 - loss: 6.1756 - acc: 0.45 - ETA: 1:42:17 - loss: 6.1901 - acc: 0.45 - ETA: 1:41:02 - loss: 6.1912 - acc: 0.45 - ETA: 1:39:46 - loss: 6.1955 - acc: 0.45 - ETA: 1:38:30 - loss: 6.2033 - acc: 0.45 - ETA: 1:37:16 - loss: 6.2148 - acc: 0.45 - ETA: 1:36:01 - loss: 6.2121 - acc: 0.45 - ETA: 1:34:46 - loss: 6.2057 - acc: 0.45 - ETA: 1:33:32 - loss: 6.2030 - acc: 0.45 - ETA: 1:32:19 - loss: 6.1964 - acc: 0.45 - ETA: 1:31:08 - loss: 6.1859 - acc: 0.45 - ETA: 1:29:56 - loss: 6.1681 - acc: 0.45 - ETA: 1:28:44 - loss: 6.1786 - acc: 0.45 - ETA: 1:27:42 - loss: 6.1922 - acc: 0.45 - ETA: 1:26:38 - loss: 6.1758 - acc: 0.45 - ETA: 1:25:34 - loss: 6.1677 - acc: 0.45 - ETA: 1:24:35 - loss: 6.1758 - acc: 0.45 - ETA: 1:23:37 - loss: 6.1531 - acc: 0.45 - ETA: 1:22:33 - loss: 6.1504 - acc: 0.45 - ETA: 1:21:30 - loss: 6.1409 - acc: 0.45 - ETA: 1:20:28 - loss: 6.1516 - acc: 0.45 - ETA: 1:19:26 - loss: 6.1491 - acc: 0.45 - ETA: 1:18:20 - loss: 6.1616 - acc: 0.45 - ETA: 1:17:11 - loss: 6.1714 - acc: 0.45 - ETA: 1:16:03 - loss: 6.1591 - acc: 0.45 - ETA: 1:14:54 - loss: 6.1567 - acc: 0.45 - ETA: 1:13:46 - loss: 6.1602 - acc: 0.45 - ETA: 1:12:41 - loss: 6.1550 - acc: 0.45 - ETA: 1:11:34 - loss: 6.1551 - acc: 0.45 - ETA: 1:10:27 - loss: 6.1601 - acc: 0.45 - ETA: 1:09:21 - loss: 6.1608 - acc: 0.45 - ETA: 1:08:14 - loss: 6.1619 - acc: 0.45 - ETA: 1:07:08 - loss: 6.1660 - acc: 0.45 - ETA: 1:06:02 - loss: 6.1702 - acc: 0.45 - ETA: 1:04:57 - loss: 6.1764 - acc: 0.45 - ETA: 1:03:52 - loss: 6.1889 - acc: 0.45 - ETA: 1:02:47 - loss: 6.1952 - acc: 0.45 - ETA: 1:01:44 - loss: 6.1903 - acc: 0.45 - ETA: 1:00:41 - loss: 6.1877 - acc: 0.45 - ETA: 59:39 - loss: 6.1883 - acc: 0.4527 - ETA: 58:36 - loss: 6.1870 - acc: 0.45 - ETA: 57:35 - loss: 6.1877 - acc: 0.45 - ETA: 56:33 - loss: 6.1893 - acc: 0.45 - ETA: 55:31 - loss: 6.1788 - acc: 0.45 - ETA: 54:30 - loss: 6.1934 - acc: 0.45 - ETA: 53:29 - loss: 6.1961 - acc: 0.45 - ETA: 52:32 - loss: 6.1963 - acc: 0.45 - ETA: 51:30 - loss: 6.1858 - acc: 0.45 - ETA: 50:31 - loss: 6.1808 - acc: 0.45 - ETA: 49:30 - loss: 6.1679 - acc: 0.45 - ETA: 48:30 - loss: 6.1525 - acc: 0.45 - ETA: 47:30 - loss: 6.1406 - acc: 0.45 - ETA: 46:30 - loss: 6.1410 - acc: 0.45 - ETA: 45:34 - loss: 6.1420 - acc: 0.45 - ETA: 44:36 - loss: 6.1451 - acc: 0.45 - ETA: 43:36 - loss: 6.1508 - acc: 0.45 - ETA: 42:37 - loss: 6.1539 - acc: 0.45 - ETA: 41:37 - loss: 6.1522 - acc: 0.45 - ETA: 40:38 - loss: 6.1546 - acc: 0.45 - ETA: 39:33 - loss: 6.1631 - acc: 0.45 - ETA: 38:36 - loss: 6.1609 - acc: 0.45 - ETA: 37:38 - loss: 6.1636 - acc: 0.45 - ETA: 36:37 - loss: 6.1623 - acc: 0.45 - ETA: 35:38 - loss: 6.1651 - acc: 0.45 - ETA: 34:38 - loss: 6.1614 - acc: 0.45 - ETA: 33:38 - loss: 6.1696 - acc: 0.45 - ETA: 32:38 - loss: 6.1663 - acc: 0.45 - ETA: 31:39 - loss: 6.1598 - acc: 0.45 - ETA: 30:40 - loss: 6.1562 - acc: 0.45 - ETA: 29:41 - loss: 6.1547 - acc: 0.45 - ETA: 28:42 - loss: 6.1609 - acc: 0.45 - ETA: 27:44 - loss: 6.1592 - acc: 0.45 - ETA: 26:45 - loss: 6.1692 - acc: 0.45 - ETA: 25:47 - loss: 6.1761 - acc: 0.45 - ETA: 24:49 - loss: 6.1677 - acc: 0.45 - ETA: 23:51 - loss: 6.1726 - acc: 0.45 - ETA: 22:54 - loss: 6.1658 - acc: 0.45 - ETA: 21:56 - loss: 6.1571 - acc: 0.45 - ETA: 20:59 - loss: 6.1641 - acc: 0.45 - ETA: 20:01 - loss: 6.1707 - acc: 0.45 - ETA: 19:04 - loss: 6.1663 - acc: 0.45 - ETA: 18:07 - loss: 6.1727 - acc: 0.45 - ETA: 17:11 - loss: 6.1738 - acc: 0.45 - ETA: 16:14 - loss: 6.1662 - acc: 0.45 - ETA: 15:18 - loss: 6.1708 - acc: 0.45 - ETA: 14:21 - loss: 6.1708 - acc: 0.45 - ETA: 13:25 - loss: 6.1755 - acc: 0.45 - ETA: 12:29 - loss: 6.1735 - acc: 0.45 - ETA: 11:34 - loss: 6.1712 - acc: 0.45 - ETA: 10:39 - loss: 6.1609 - acc: 0.45 - ETA: 9:45 - loss: 6.1559 - acc: 0.4522 - ETA: 8:50 - loss: 6.1587 - acc: 0.452 - ETA: 7:55 - loss: 6.1627 - acc: 0.451 - ETA: 7:00 - loss: 6.1634 - acc: 0.451 - ETA: 6:05 - loss: 6.1602 - acc: 0.452 - ETA: 5:10 - loss: 6.1552 - acc: 0.451 - ETA: 4:15 - loss: 6.1555 - acc: 0.451202/201 [==============================] - ETA: 3:19 - loss: 6.1598 - acc: 0.451 - ETA: 2:24 - loss: 6.1576 - acc: 0.451 - ETA: 1:28 - loss: 6.1559 - acc: 0.452 - ETA: 33s - loss: 6.1499 - acc: 0.452 - 14283s 71s/step - loss: 6.1501 - acc: 0.4523 - val_loss: 2.3895 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.38949, saving model to weights.best.RESN50.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights.best.RESN50.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "History = Chest_Dens_121_model.fit_generator(train_generator, \n",
    "                                        epochs=1,\n",
    "                                        validation_data = test_generator,\n",
    "                                        validation_steps = test_generator.samples / test_generator.batch_size,  \n",
    "                                        steps_per_epoch = train_generator.samples / train_generator.batch_size, \n",
    "                                        callbacks=[checkpointer], verbose=1, shuffle=False)\n",
    "#use_multiprocessing=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-3ae0c0d2b6de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1968\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1970\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m   1922\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m                            interpolation=self.interpolation)\n\u001b[1;32m-> 1924\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m             \u001b[1;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m             \u001b[1;31m# but not PIL images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;31m# or (channel, height, width)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# but original PIL image has format (width, height, channel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features, label in train_generator:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, img_width, img_height, 1)\n",
    "\n",
    "pickle_out = pickle.open('X_DENSNET.pickle', \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = pickle.open('y_DENSNET.pickle', \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/4032 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 5/4032 [00:00<01:42, 39.16it/s]\n",
      "\n",
      "  0%|          | 10/4032 [00:00<01:37, 41.29it/s]\n",
      "\n",
      "  0%|          | 15/4032 [00:00<01:34, 42.42it/s]\n",
      "\n",
      "  0%|          | 20/4032 [00:00<01:31, 43.99it/s]\n",
      "\n",
      "  1%|          | 25/4032 [00:00<01:30, 44.34it/s]\n",
      "\n",
      "  1%|          | 30/4032 [00:00<01:30, 44.11it/s]\n",
      "\n",
      "  1%|          | 35/4032 [00:00<01:28, 44.96it/s]\n",
      "\n",
      "  1%|          | 40/4032 [00:00<01:31, 43.84it/s]\n",
      "\n",
      "  1%|          | 45/4032 [00:01<01:28, 45.19it/s]\n",
      "\n",
      "  1%|          | 50/4032 [00:01<01:26, 46.05it/s]\n",
      "\n",
      "  1%|▏         | 55/4032 [00:01<01:25, 46.51it/s]\n",
      "\n",
      "  1%|▏         | 60/4032 [00:01<01:24, 47.00it/s]\n",
      "\n",
      "  2%|▏         | 65/4032 [00:01<01:32, 43.09it/s]\n",
      "\n",
      "  2%|▏         | 70/4032 [00:01<01:36, 41.22it/s]\n",
      "\n",
      "  2%|▏         | 75/4032 [00:01<01:34, 41.77it/s]\n",
      "\n",
      "  2%|▏         | 80/4032 [00:01<01:32, 42.62it/s]\n",
      "\n",
      "  2%|▏         | 85/4032 [00:01<01:37, 40.52it/s]\n",
      "\n",
      "  2%|▏         | 90/4032 [00:02<01:42, 38.62it/s]\n",
      "\n",
      "  2%|▏         | 95/4032 [00:02<01:39, 39.70it/s]\n",
      "\n",
      "  2%|▏         | 100/4032 [00:02<01:36, 40.70it/s]\n",
      "\n",
      "  3%|▎         | 105/4032 [00:02<01:34, 41.42it/s]\n",
      "\n",
      "  3%|▎         | 110/4032 [00:02<01:31, 43.03it/s]\n",
      "\n",
      "  3%|▎         | 115/4032 [00:02<01:31, 42.64it/s]\n",
      "\n",
      "  3%|▎         | 121/4032 [00:02<01:27, 44.65it/s]\n",
      "\n",
      "  3%|▎         | 126/4032 [00:02<01:25, 45.77it/s]\n",
      "\n",
      "  3%|▎         | 131/4032 [00:03<01:29, 43.43it/s]\n",
      "\n",
      "  3%|▎         | 136/4032 [00:03<01:32, 41.95it/s]\n",
      "\n",
      "  3%|▎         | 141/4032 [00:03<01:40, 38.53it/s]\n",
      "\n",
      "  4%|▎         | 146/4032 [00:03<01:36, 40.21it/s]\n",
      "\n",
      "  4%|▎         | 151/4032 [00:03<01:33, 41.68it/s]\n",
      "\n",
      "  4%|▍         | 157/4032 [00:03<01:28, 43.61it/s]\n",
      "\n",
      "  4%|▍         | 162/4032 [00:03<01:27, 44.30it/s]\n",
      "\n",
      "  4%|▍         | 167/4032 [00:03<01:27, 44.08it/s]\n",
      "\n",
      "  4%|▍         | 172/4032 [00:03<01:26, 44.64it/s]\n",
      "\n",
      "  4%|▍         | 177/4032 [00:04<01:24, 45.41it/s]\n",
      "\n",
      "  5%|▍         | 182/4032 [00:04<01:25, 45.00it/s]\n",
      "\n",
      "  5%|▍         | 187/4032 [00:04<01:24, 45.39it/s]\n",
      "\n",
      "  5%|▍         | 192/4032 [00:04<01:22, 46.46it/s]\n",
      "\n",
      "  5%|▍         | 197/4032 [00:04<01:25, 44.70it/s]\n",
      "\n",
      "  5%|▌         | 202/4032 [00:04<01:24, 45.57it/s]\n",
      "\n",
      "  5%|▌         | 207/4032 [00:04<01:25, 44.96it/s]\n",
      "\n",
      "  5%|▌         | 212/4032 [00:04<01:23, 45.74it/s]\n",
      "\n",
      "  5%|▌         | 217/4032 [00:04<01:26, 44.35it/s]"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d754ee24f20b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# pre-process the data for Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtest_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-d754ee24f20b>\u001b[0m in \u001b[0;36mpaths_to_tensor\u001b[1;34m(img_paths)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mlist_of_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-d754ee24f20b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mlist_of_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-d754ee24f20b>\u001b[0m in \u001b[0;36mpath_to_tensor\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m365\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m365\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;31m# or (channel, height, width)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# but original PIL image has format (width, height, channel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|▌         | 217/4032 [00:23<01:26, 44.35it/s]"
     ]
    }
   ],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(365, 365))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((967, 224, 224, 3), (967, 2))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensors.shape, test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected densenet121_input to have 4 dimensions, but got array with shape (1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c3006fcf6e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m### Calculate classification accuracy on the test dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# get index of predicted dog breed for each image in test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChest_Dens_121_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# report test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-c3006fcf6e1b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m### Calculate classification accuracy on the test dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# get index of predicted dog breed for each image in test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChest_Dens_121_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# report test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected densenet121_input to have 4 dimensions, but got array with shape (1, 1)"
     ]
    }
   ],
   "source": [
    "### Load the model weights with the best validation loss.\n",
    "Chest_Dens_121_model.load_weights('weights.best.RESN50.hdf5')\n",
    "\n",
    "### Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "predictions = [np.argmax(Chest_Dens_121_model.predict(np.expand_dims(feature, axis=0))) for feature in test_files]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,10))  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "\n",
    "plt.subplot(211)  \n",
    "plt.plot(History.history['acc'])  \n",
    "plt.plot(History.history['val_acc'])  \n",
    "plt.title('Model Accuracy')  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  \n",
    "\n",
    "# summarize history for loss  \n",
    "\n",
    "plt.subplot(212)  \n",
    "plt.plot(History.history['loss'])  \n",
    "plt.plot(History.history['val_loss'])  \n",
    "plt.title('Model Loss')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
