{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:02.250633Z",
     "start_time": "2019-01-09T17:11:39.401674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from finalproject.class_dataset import ChestDataset\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.applications import DenseNet121\n",
    "from keras import models\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:04.265498Z",
     "start_time": "2019-01-09T17:12:04.148181Z"
    }
   },
   "outputs": [],
   "source": [
    "csvfile = 'finalproject/data/Data_Entry_2017.csv'\n",
    "train_list = open('finalproject/data/train_val_list.txt','r').read().split('\\n')\n",
    "test_list = open('finalproject/data/test_list.txt','r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:05.070520Z",
     "start_time": "2019-01-09T17:12:04.659128Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:05.545502Z",
     "start_time": "2019-01-09T17:12:05.422471Z"
    }
   },
   "outputs": [],
   "source": [
    "df_uni = df[~df['Finding Labels'].str.contains('\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:09.661029Z",
     "start_time": "2019-01-09T17:12:06.009364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yair/Documents/ITC/Final/finalproject/class_dataset.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.reader['exists'] = self.exists\n",
      "/home/yair/Documents/ITC/Final/finalproject/class_dataset.py:139: UserWarning: file not found\n",
      "  warnings.warn('file not found')\n",
      "/home/yair/Documents/ITC/Final/finalproject/class_dataset.py:114: UserWarning: Image not found in folder\n",
      "  warnings.warn('Image not found in folder')\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'image/images/'\n",
    "dataset = ChestDataset(data_dir,df_uni,4)\n",
    "dataset.reset_folder()\n",
    "dataset = ChestDataset(data_dir,df_uni,4)\n",
    "idx = [i for i,el in enumerate(dataset.exists) if el == True]\n",
    "labels = [dataset.labels[i] for i, el in enumerate(dataset.exists) if el == True]\n",
    "train_dt,test_dt = dataset.train_test(train_list,test_list)\n",
    "train_dt.create_tree()\n",
    "test_dt.create_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:10.332555Z",
     "start_time": "2019-01-09T17:12:10.329384Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add GPU options\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.333)\n",
    "# sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n",
    "# NAME = 'Binary_chest_cnn_64x3_{}'.format(int(time.time()))\n",
    "# tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "# #tensorboard --logdir=logs/ --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:10.942249Z",
     "start_time": "2019-01-09T17:12:10.936160Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = train_dt.image_path\n",
    "test_files = test_dt.image_path\n",
    "train_folder = train_dt.dir\n",
    "test_folder = test_dt.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:11.516058Z",
     "start_time": "2019-01-09T17:12:11.503009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train # No Finding: 0.7090673094752457\n",
      "Test # No Finding: 0.5034446210916799\n"
     ]
    }
   ],
   "source": [
    "label_train = [train_dt.labels[i] for i, el in enumerate(train_dt.exists) if el == True]\n",
    "label_test = [test_dt.labels[i] for i, el in enumerate(test_dt.exists) if el == True]\n",
    "print('Train # No Finding:',label_train.count('No Finding')/len(label_train))\n",
    "print('Test # No Finding:',label_test.count('No Finding')/len(label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:12.132212Z",
     "start_time": "2019-01-09T17:12:12.106764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about the Dataset:\n",
      "\n",
      "There are 15 total chest deseases.\n",
      "There are 12673 total chest images.\n",
      "\n",
      "There are 10786 training chest images.\n",
      "There are 1887 test chest images.\n",
      "# of Infiltration: 8.672%\n",
      "# of Mass: 1.941%\n",
      "# of Effusion: 3.803%\n",
      "# of Hernia: 0.221%\n",
      "# of Fibrosis: 1.452%\n",
      "# of Emphysema: 0.955%\n",
      "# of Pneumothorax: 1.712%\n",
      "# of Consolidation: 1.499%\n",
      "# of Pneumonia: 0.308%\n",
      "# of Atelectasis: 4.569%\n",
      "# of Pleural_Thickening: 1.278%\n",
      "# of Edema: 0.529%\n",
      "# of No Finding: 67.845%\n",
      "# of Nodule: 3.133%\n",
      "# of Cardiomegaly: 2.083%\n"
     ]
    }
   ],
   "source": [
    "print('Statistics about the Dataset:\\n')\n",
    "print('There are %d total chest deseases.' % len(set(dataset.labels)))\n",
    "print('There are %s total chest images.\\n' % np.sum(dataset.exists))\n",
    "print('There are %d training chest images.' % np.sum(train_dt.exists))\n",
    "# print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test chest images.'% np.sum(test_dt.exists))\n",
    "for lab in set(labels):\n",
    "    print('# of %s: %.3f%%'%(lab,100*labels.count(lab)/len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T17:12:52.086479Z",
     "start_time": "2019-01-09T17:12:15.791713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 11, 11, 1024)      7037504   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 123904)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                1858575   \n",
      "=================================================================\n",
      "Total params: 8,896,079\n",
      "Trainable params: 1,858,575\n",
      "Non-trainable params: 7,037,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_width,img_height = 365,365\n",
    "densenet = DenseNet121(weights='imagenet', include_top=False,input_shape = (img_width, img_height, 3))\n",
    "\n",
    "# Freeze some layers\n",
    "for layer in densenet.layers[:]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "\n",
    "model.add(densenet)\n",
    "\n",
    "# Add new layers\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(72))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.248))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-09T17:13:11.789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10786 images belonging to 15 classes.\n",
      "Found 1887 images belonging to 15 classes.\n",
      "Epoch 1/2\n",
      " 208/1078 [====>.........................] - ETA: 1:58:34 - loss: 0.6356 - acc: 0.9601"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 10\n",
    "val_batchsize = 10\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=train_batchsize,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    test_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=val_batchsize,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "prediction = model.predict_generator(validation_generator,\n",
    "                                    validation_generator.samples // validation_generator.batch_size,\n",
    "                                    verbose=1,\n",
    "                                    use_multiprocessing=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
